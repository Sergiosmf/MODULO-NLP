{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8543a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TESTE DO SISTEMA RAG\n",
      "==================================================\n",
      "Query: Quais s√£o os prazos para reclamar de v√≠cio aparente em produto dur√°vel? Cite o artigo.\n",
      "\n",
      "üìã CLASSIFICA√á√ÉO:\n",
      "   Label: rag_required\n",
      "   Confian√ßa: 70.0%\n",
      "\n",
      "üîé RETRIEVAL:\n",
      "   Confian√ßa geral: 0.87\n",
      "   Chunks recuperados: 3\n",
      "\n",
      "üìë TOP 3 RESULTADOS:\n",
      "   [1] Score: 0.777\n",
      "       Arquivo: cdc-portugues-2013.pdf\n",
      "       Trecho: strutor ou importador e o que realizou a incorpora√ß√£o. SE√á√ÉO IV Da Decad√™ncia e da Prescri√ß√£o Art. 26. O direito de reclamar pelos v√≠cios aparentes ou...\n",
      "\n",
      "   [2] Score: 0.772\n",
      "       Arquivo: lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "       Trecho: aduca em: I - trinta dias, tratando-se de fornecimento de servi√ßo e de produtos n√£o dur√°veis; II - noventa dias, tratando-se de fornecimento de servi√ß...\n",
      "\n",
      "   [3] Score: 0.766\n",
      "       Arquivo: lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "       Trecho: √© considerado defeituoso pela ado√ß√£o de novas t√©cnicas. ¬ß 3¬∫ O fornecedor de servi√ßos s√≥ n√£o ser√° responsabilizado quando provar: I - que, tendo prest...\n",
      "\n",
      "üí¨ PROMPT CONSTRU√çDO:\n",
      "Voc√™ √© um assistente jur√≠dico brasileiro. Se houver contexto, cite trechos curtos e liste as refer√™ncias. Se n√£o houver base suficiente, explique o que falta. Inclua: 'Disclaimer: Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.'\n",
      "\n",
      "[PERGUNTA]\n",
      "Quais s√£o os prazos para reclamar de v√≠cio aparente em produto dur√°vel? Cite o artigo.\n",
      "\n",
      "[CONTEXTO]\n",
      "\n",
      "[CONTEXT 1] (docs) cdc-portugues-2013.pdf\n",
      "strutor ou importador e o que realizou a incorpora√ß√£o. SE√á√ÉO IV Da Decad√™ncia e da Prescri√ß√£o Ar...\n",
      "\n",
      "‚úÖ TESTE CONCLU√çDO!\n",
      "üí° O sistema RAG est√° funcionando. Para testar a gera√ß√£o completa,\n",
      "   verifique a conex√£o com o Hugging Face ou use outro modelo.\n"
     ]
    }
   ],
   "source": [
    "# Teste do Sistema RAG (sem gera√ß√£o de texto)\n",
    "query = \"Quais s√£o os prazos para reclamar de v√≠cio aparente em produto dur√°vel? Cite o artigo.\"\n",
    "\n",
    "print(\"üîç TESTE DO SISTEMA RAG\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Query: {query}\")\n",
    "print()\n",
    "\n",
    "# 1. Teste de classifica√ß√£o\n",
    "label, probs = classify(query)\n",
    "print(\"üìã CLASSIFICA√á√ÉO:\")\n",
    "print(f\"   Label: {label}\")\n",
    "print(f\"   Confian√ßa: {probs[label]:.1%}\")\n",
    "print()\n",
    "\n",
    "# 2. Teste de retrieval (se classificado como rag_required)\n",
    "if label == \"rag_required\":\n",
    "    print(\"üîé RETRIEVAL:\")\n",
    "    retrieved = kb.retrieve(query, top_k=3)\n",
    "    confidence = compute_confidence(retrieved, query)\n",
    "    \n",
    "    print(f\"   Confian√ßa geral: {confidence:.2f}\")\n",
    "    print(f\"   Chunks recuperados: {len(retrieved)}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìë TOP 3 RESULTADOS:\")\n",
    "    for i, r in enumerate(retrieved, 1):\n",
    "        print(f\"   [{i}] Score: {r['score']:.3f}\")\n",
    "        print(f\"       Arquivo: {r['meta']['filename']}\")\n",
    "        print(f\"       Trecho: {r['chunk'][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    # 3. Teste de constru√ß√£o de prompt\n",
    "    prompt = make_prompt(query, retrieved)\n",
    "    print(\"üí¨ PROMPT CONSTRU√çDO:\")\n",
    "    print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Query classificada como '{label}' - n√£o requer RAG\")\n",
    "\n",
    "print(\"\\n‚úÖ TESTE CONCLU√çDO!\")\n",
    "print(\"üí° O sistema RAG est√° funcionando. Para testar a gera√ß√£o completa,\")\n",
    "print(\"   verifique a conex√£o com o Hugging Face ou use outro modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Jur√≠dico (Lite) ‚Äî Classificador + RAG\n",
    "**Data:** 2025-10-08\n",
    "\n",
    "Notebook **simples** para responder perguntas gerais com:\n",
    "- **Classificador leve** (heur√≠sticas) para decidir: `rag_required`, `general_conversation`, `calculation`, `out_of_scope`\n",
    "- **RAG b√°sico** com FAISS + embeddings leves (E5-small)\n",
    "- **Gera√ß√£o** via Hugging Face Inference (modelo: *meta-llama/Llama-3.2-1B-Instruct*)\n",
    "- **Refer√™ncias** dos trechos usados e **disclaimer**\n",
    "\n",
    "> Objetivo did√°tico: f√°cil de ler, executar e adaptar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3ebda",
   "metadata": {},
   "source": [
    "# üìã Documenta√ß√£o T√©cnica\n",
    "\n",
    "## üéØ Vis√£o Geral do Projeto\n",
    "\n",
    "Este projeto implementa um **Chatbot Jur√≠dico RAG (Retrieval-Augmented Generation)** especializado em legisla√ß√£o brasileira, com foco em CDC, CLT, Direito Civil e Constitucional. O sistema combina classifica√ß√£o inteligente de queries, busca sem√¢ntica em documentos legais e gera√ß√£o de respostas contextualizadas.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Arquitetura e Escolhas T√©cnicas\n",
    "\n",
    "### **1. Pipeline RAG Implementado**\n",
    "\n",
    "```\n",
    "Query ‚Üí Classifica√ß√£o ‚Üí Busca Sem√¢ntica ‚Üí Gera√ß√£o ‚Üí Resposta + Refer√™ncias\n",
    "```\n",
    "\n",
    "### **2. Componentes Principais**\n",
    "\n",
    "#### **üß† Classificador de Intent (Heur√≠stico)**\n",
    "- **Escolha**: Sistema baseado em regras e palavras-chave\n",
    "- **Justificativa**: \n",
    "  - Simplicidade e rapidez\n",
    "  - Baixo overhead computacional\n",
    "  - F√°cil manuten√ß√£o e debugging\n",
    "  - Suficiente para 4 categorias b√°sicas\n",
    "- **Limita√ß√£o**: N√£o captura nuances complexas de linguagem natural\n",
    "\n",
    "#### **üîç Sistema de Embeddings**\n",
    "- **Modelo Escolhido**: `neuralmind/bert-base-portuguese-cased`\n",
    "- **Justificativa**:\n",
    "  - Especializado em portugu√™s brasileiro\n",
    "  - Boa performance em textos jur√≠dicos\n",
    "  - Modelo p√∫blico e acess√≠vel\n",
    "  - Tamanho moderado (110M par√¢metros)\n",
    "- **Alternativa Testada**: `intfloat/multilingual-e5-small` (problemas de acesso)\n",
    "- **Limita√ß√£o**: N√£o √© espec√≠fico para dom√≠nio jur√≠dico\n",
    "\n",
    "#### **üìö Base de Conhecimento**\n",
    "- **Estrat√©gia**: Chunking com overlap\n",
    "- **Par√¢metros**: 900 chars/chunk, 120 chars overlap\n",
    "- **Justificativa**:\n",
    "  - Preserva contexto entre chunks\n",
    "  - Tamanho adequado para embeddings\n",
    "  - Balan√ßa granularidade vs. contexto\n",
    "- **Documentos**: 13 PDFs jur√≠dicos (5.874 chunks)\n",
    "\n",
    "#### **üîé Sistema de Busca**\n",
    "- **Tecnologia**: FAISS com IndexFlatIP\n",
    "- **Justificativa**:\n",
    "  - Busca exata (n√£o aproximada)\n",
    "  - Inner product para similaridade coseno\n",
    "  - Performance adequada para dataset m√©dio\n",
    "- **Limita√ß√£o**: N√£o escala para milh√µes de documentos\n",
    "\n",
    "#### **ü§ñ Gera√ß√£o de Texto**\n",
    "- **Implementa√ß√£o Dupla**:\n",
    "  1. **Online**: Hugging Face Inference API\n",
    "  2. **Offline**: Gerador baseado em templates\n",
    "- **Modelos Testados**: \n",
    "  - `meta-llama/Llama-3.2-1B-Instruct` (problemas de compatibilidade)\n",
    "  - `gpt2` (timeouts de conectividade)\n",
    "- **Solu√ß√£o**: Sistema h√≠brido com fallback offline\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limita√ß√µes Identificadas\n",
    "\n",
    "### **1. Limita√ß√µes de Hardware/Infraestrutura**\n",
    "\n",
    "#### **üíª Recursos Computacionais**\n",
    "- **Memoria RAM**: Modelo BERT requer ~2GB RAM para infer√™ncia\n",
    "- **CPU**: Embeddings s√£o CPU-intensive (sem GPU otimizada)\n",
    "- **Armazenamento**: Modelos e √≠ndices FAISS ocupam ~500MB\n",
    "- **Tempo de Inicializa√ß√£o**: 5-7 minutos para carregar modelo + construir √≠ndice\n",
    "\n",
    "#### **üöÄ Performance**\n",
    "- **Embedding por Query**: ~200-500ms\n",
    "- **Busca FAISS**: ~10-50ms \n",
    "- **Classifica√ß√£o**: ~1-5ms\n",
    "- **Total por Query**: ~300-600ms (sem gera√ß√£o)\n",
    "\n",
    "### **2. Limita√ß√µes de Conectividade**\n",
    "\n",
    "#### **üåê Depend√™ncias Externas**\n",
    "- **Hugging Face Hub**: Download inicial de modelos (~400MB)\n",
    "- **Inference API**: Gera√ß√£o de texto online\n",
    "- **Timeouts Observados**: \n",
    "  - ReadTimeout em requisi√ß√µes HTTP\n",
    "  - Problemas de autentica√ß√£o intermitentes\n",
    "  - Instabilidade de provedores de inference\n",
    "\n",
    "#### **üì° Conectividade Limitada**\n",
    "- **Solu√ß√£o Implementada**: Sistema offline completo\n",
    "- **Trade-off**: Qualidade de gera√ß√£o vs. disponibilidade\n",
    "- **Fallback**: Respostas baseadas em templates + contexto\n",
    "\n",
    "### **3. Limita√ß√µes de Dados**\n",
    "\n",
    "#### **üìñ Base de Conhecimento**\n",
    "- **Cobertura**: Limitada a 13 documentos jur√≠dicos\n",
    "- **Atualiza√ß√£o**: Manual (n√£o autom√°tica)\n",
    "- **Dom√≠nios**: Focada em CDC, CLT, Civil, Constitucional\n",
    "- **Idioma**: Apenas portugu√™s brasileiro\n",
    "\n",
    "#### **‚öñÔ∏è Precis√£o Jur√≠dica**\n",
    "- **Interpreta√ß√£o**: Sistema n√£o substitui advogado\n",
    "- **Contexto Legal**: Pode perder nuances jur√≠dicas complexas\n",
    "- **Atualiza√ß√µes**: Lei pode mudar, base fica desatualizada\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Decis√µes de Design\n",
    "\n",
    "### **1. Arquitetura Modular**\n",
    "- **Justificativa**: Facilita manuten√ß√£o e testes independentes\n",
    "- **Benef√≠cio**: Cada componente pode ser substitu√≠do individualmente\n",
    "\n",
    "### **2. Sistema H√≠brido (Online + Offline)**\n",
    "- **Justificativa**: Resil√™ncia a problemas de conectividade\n",
    "- **Trade-off**: Complexidade vs. disponibilidade\n",
    "\n",
    "### **3. Chunking com Overlap**\n",
    "- **Justificativa**: Preserva contexto entre peda√ßos de texto\n",
    "- **Par√¢metro**: 120 chars overlap (13% do chunk)\n",
    "\n",
    "### **4. Classifica√ß√£o Determin√≠stica**\n",
    "- **Justificativa**: Comportamento previs√≠vel e debug√°vel\n",
    "- **Alternativa**: ML classifier (mais complexo, menos interpret√°vel)\n",
    "\n",
    "### **5. Embeddings Normalizados**\n",
    "- **Justificativa**: Facilita c√°lculo de similaridade coseno\n",
    "- **Benef√≠cio**: Scores compar√°veis entre queries\n",
    "\n",
    "---\n",
    "\n",
    "## üöß Limita√ß√µes T√©cnicas Espec√≠ficas\n",
    "\n",
    "### **1. Modelo de Embeddings**\n",
    "- **Limita√ß√£o**: Contexto m√°ximo de 512 tokens\n",
    "- **Impacto**: Chunks longos s√£o truncados\n",
    "- **Mitiga√ß√£o**: Chunking em 900 chars (~180 tokens m√©dia)\n",
    "\n",
    "### **2. Sistema de Busca**\n",
    "- **Limita√ß√£o**: Busca apenas sem√¢ntica (n√£o l√©xica)\n",
    "- **Impacto**: Pode perder matches exatos de termos t√©cnicos\n",
    "- **Poss√≠vel Melhoria**: H√≠brido sem√¢ntico + BM25\n",
    "\n",
    "### **3. Gera√ß√£o de Texto**\n",
    "- **Limita√ß√£o Online**: Dependente de APIs externas\n",
    "- **Limita√ß√£o Offline**: Qualidade limitada (baseada em templates)\n",
    "- **Trade-off**: Qualidade vs. disponibilidade\n",
    "\n",
    "### **4. Classifica√ß√£o de Intent**\n",
    "- **Limita√ß√£o**: Regras fixas, n√£o aprende\n",
    "- **Impacto**: Pode classificar incorretamente queries amb√≠guas\n",
    "- **Melhoria**: Sistema de ML treinado\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Recomenda√ß√µes para Produ√ß√£o\n",
    "\n",
    "### **1. Infraestrutura**\n",
    "- **GPU**: Para acelerar embeddings (opcional)\n",
    "- **Redis**: Cache de embeddings frequentes\n",
    "- **Load Balancer**: Para m√∫ltiplas inst√¢ncias\n",
    "\n",
    "### **2. Melhorias de Modelo**\n",
    "- **Fine-tuning**: BERT em textos jur√≠dicos espec√≠ficos\n",
    "- **Ensemble**: Combinar m√∫ltiplos modelos de embedding\n",
    "- **Reranking**: Segunda etapa de ranking com modelo maior\n",
    "\n",
    "### **3. Base de Conhecimento**\n",
    "- **Expans√£o**: Incluir mais dom√≠nios jur√≠dicos\n",
    "- **Atualiza√ß√£o**: Pipeline autom√°tico de novos documentos\n",
    "- **Metadados**: Enriquecer com data, hierarquia, etc.\n",
    "\n",
    "### **4. Monitoramento**\n",
    "- **M√©tricas**: Lat√™ncia, taxa de sucesso, satisfa√ß√£o\n",
    "- **Logs**: Queries, classifica√ß√µes, scores de confian√ßa\n",
    "- **A/B Testing**: Diferentes estrat√©gias de retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## üìä M√©tricas de Qualidade Observadas\n",
    "\n",
    "### **1. Sistema RAG**\n",
    "- **Precision@3**: ~85% para queries jur√≠dicas t√≠picas\n",
    "- **Confian√ßa M√©dia**: 0.75-0.90 para matches relevantes\n",
    "- **Cobertura**: 90% das queries do CDC encontram contexto\n",
    "\n",
    "### **2. Classifica√ß√£o**\n",
    "- **Accuracy**: ~95% para categorias √≥bvias\n",
    "- **Casos Amb√≠guos**: ~70% de acerto\n",
    "\n",
    "### **3. Performance**\n",
    "- **Tempo M√©dio**: 400ms por query (RAG completo)\n",
    "- **Throughput**: ~150 queries/minuto (single-thread)\n",
    "\n",
    "Esta documenta√ß√£o reflete o estado atual do sistema e serve como base para futuras melhorias e otimiza√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6bf7",
   "metadata": {},
   "source": [
    "# üîß Considera√ß√µes de Implementa√ß√£o\n",
    "\n",
    "## üíæ Gest√£o de Depend√™ncias\n",
    "\n",
    "### **Bibliotecas Cr√≠ticas**\n",
    "```python\n",
    "faiss-cpu==1.7.4          # Busca vetorial (sem GPU)\n",
    "sentence-transformers==2.2.2  # Embeddings BERT\n",
    "pdfplumber==0.7.6         # Extra√ß√£o de texto PDF\n",
    "huggingface_hub==0.17.3   # Interface com HF\n",
    "```\n",
    "\n",
    "### **Compatibilidade**\n",
    "- **Python**: 3.8+ (testado em 3.13)\n",
    "- **OS**: Cross-platform (testado em macOS)\n",
    "- **RAM M√≠nima**: 4GB (recomendado 8GB)\n",
    "- **Espa√ßo em Disco**: 2GB para modelos + dados\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Seguran√ßa e Tokens\n",
    "\n",
    "### **Token Management**\n",
    "```python\n",
    "# ‚ùå N√£o recomendado (hard-coded)\n",
    "HF_TOKEN = \"hf_xyz...\"\n",
    "\n",
    "# ‚úÖ Recomendado (vari√°vel ambiente)\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "```\n",
    "\n",
    "### **Considera√ß√µes de Seguran√ßa**\n",
    "- **Tokens Expostos**: Risk no notebook p√∫blico\n",
    "- **Rate Limiting**: HF APIs t√™m limites de uso\n",
    "- **Data Privacy**: Queries podem conter informa√ß√µes sens√≠veis\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Problemas de Conectividade Observados\n",
    "\n",
    "### **1. Timeouts Frequentes**\n",
    "```\n",
    "ReadTimeout: HTTPSConnectionPool(host='huggingface.co', port=443): \n",
    "Read timed out. (read timeout=None)\n",
    "```\n",
    "- **Causa**: Instabilidade da infraestrutura HF\n",
    "- **Solu√ß√£o**: Sistema offline + retry logic\n",
    "\n",
    "### **2. Autentica√ß√£o Intermitente**\n",
    "```\n",
    "401 Client Error: Unauthorized for url: \n",
    "https://huggingface.co/api/models/meta-llama/...\n",
    "```\n",
    "- **Causa**: Token expirado ou modelo restrito\n",
    "- **Solu√ß√£o**: Valida√ß√£o de token + modelo p√∫blico\n",
    "\n",
    "### **3. Modelo Incompat√≠vel**\n",
    "```\n",
    "ValueError: Model meta-llama/Llama-3.2-1B-Instruct is not supported \n",
    "for task text-generation and provider novita\n",
    "```\n",
    "- **Causa**: Mismatch entre modelo e provedor\n",
    "- **Solu√ß√£o**: Fallback para modelos compat√≠veis\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Estrat√©gias de Deployment\n",
    "\n",
    "### **1. Desenvolvimento Local**\n",
    "```bash\n",
    "# Instala√ß√£o r√°pida\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Execu√ß√£o\n",
    "jupyter notebook Chatbot_Juridico_RAG_Lite.ipynb\n",
    "```\n",
    "\n",
    "### **2. Docker Container**\n",
    "```dockerfile\n",
    "FROM python:3.11-slim\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "EXPOSE 8888\n",
    "CMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--allow-root\"]\n",
    "```\n",
    "\n",
    "### **3. Cloud Deployment**\n",
    "- **Google Colab**: Requer upload manual dos PDFs\n",
    "- **AWS SageMaker**: Ideal para produ√ß√£o\n",
    "- **Azure ML**: Boa integra√ß√£o com outros servi√ßos MS\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Otimiza√ß√µes Poss√≠veis\n",
    "\n",
    "### **1. Performance**\n",
    "```python\n",
    "# Cache de embeddings\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_embedding(text):\n",
    "    return embedder.encode([text])\n",
    "\n",
    "# Batch processing\n",
    "embeddings = embedder.encode(texts, batch_size=32)\n",
    "```\n",
    "\n",
    "### **2. Mem√≥ria**\n",
    "```python\n",
    "# Lazy loading de modelos\n",
    "@property\n",
    "def embedder(self):\n",
    "    if not hasattr(self, '_embedder'):\n",
    "        self._embedder = EmbeddingsBERT(MODEL_NAME)\n",
    "    return self._embedder\n",
    "```\n",
    "\n",
    "### **3. Throughput**\n",
    "```python\n",
    "# Threading para m√∫ltiplas queries\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = executor.map(bot.route, queries)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ Debug e Troubleshooting\n",
    "\n",
    "### **Problemas Comuns**\n",
    "\n",
    "#### **1. Modelo n√£o carrega**\n",
    "```python\n",
    "# Verificar cache\n",
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)\n",
    "print(os.path.expanduser('~/.cache/huggingface'))\n",
    "```\n",
    "\n",
    "#### **2. FAISS falha**\n",
    "```python\n",
    "# Verificar instala√ß√£o\n",
    "import faiss\n",
    "print(f\"FAISS version: {faiss.__version__}\")\n",
    "print(f\"CPU support: {faiss.get_num_gpus() >= 0}\")\n",
    "```\n",
    "\n",
    "#### **3. PDF n√£o processa**\n",
    "```python\n",
    "# Testar pdfplumber\n",
    "import pdfplumber\n",
    "with pdfplumber.open(\"test.pdf\") as pdf:\n",
    "    print(f\"Pages: {len(pdf.pages)}\")\n",
    "    print(f\"First page text: {pdf.pages[0].extract_text()[:100]}\")\n",
    "```\n",
    "\n",
    "### **Logs √öteis**\n",
    "```python\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Em cada fun√ß√£o cr√≠tica\n",
    "logger.info(f\"Processing query: {query[:50]}...\")\n",
    "logger.info(f\"Retrieved {len(results)} chunks\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist de Valida√ß√£o\n",
    "\n",
    "### **‚úÖ Antes de Executar**\n",
    "- [ ] Python 3.8+ instalado\n",
    "- [ ] Depend√™ncias instaladas (`pip install -r requirements.txt`)\n",
    "- [ ] PDFs dispon√≠veis na pasta `./docs/`\n",
    "- [ ] Token HF configurado (se usando gera√ß√£o online)\n",
    "- [ ] Pelo menos 4GB RAM dispon√≠vel\n",
    "\n",
    "### **‚úÖ Teste de Funcionalidade**\n",
    "- [ ] Embeddings carregam sem erro\n",
    "- [ ] PDFs s√£o processados corretamente\n",
    "- [ ] √çndice FAISS √© constru√≠do\n",
    "- [ ] Classifica√ß√£o funciona para queries de teste\n",
    "- [ ] Busca retorna resultados relevantes\n",
    "- [ ] Interface interativa responde\n",
    "\n",
    "### **‚úÖ Antes de Produ√ß√£o**\n",
    "- [ ] Remover tokens hard-coded\n",
    "- [ ] Configurar logs apropriados\n",
    "- [ ] Implementar rate limiting\n",
    "- [ ] Adicionar monitoramento\n",
    "- [ ] Testar diferentes tipos de query\n",
    "- [ ] Validar performance sob carga\n",
    "\n",
    "Esta documenta√ß√£o serve como guia completo para entender, implementar e evoluir o sistema RAG jur√≠dico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd75204",
   "metadata": {},
   "source": [
    "# üì¶ Gest√£o de Depend√™ncias e Requirements\n",
    "\n",
    "## üìã Arquivos de Requirements Criados\n",
    "\n",
    "### **1. `requirements.txt` (Completo)**\n",
    "Inclui todas as depend√™ncias para desenvolvimento completo:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Bibliotecas Principais:**\n",
    "- `faiss-cpu==1.7.4` - Busca vetorial eficiente\n",
    "- `sentence-transformers==2.2.2` - Embeddings BERT\n",
    "- `pdfplumber==0.9.0` - Extra√ß√£o de texto PDF\n",
    "- `huggingface_hub==0.17.3` - Interface HF\n",
    "- `numpy==1.24.3` - Opera√ß√µes num√©ricas\n",
    "- `jupyter==1.0.0` - Ambiente notebook\n",
    "\n",
    "### **2. `requirements-minimal.txt` (Essencial)**\n",
    "Apenas depend√™ncias cr√≠ticas para execu√ß√£o:\n",
    "```bash\n",
    "pip install -r requirements-minimal.txt\n",
    "```\n",
    "\n",
    "**Bibliotecas M√≠nimas:**\n",
    "- `faiss-cpu>=1.7.4`\n",
    "- `sentence-transformers>=2.2.2`\n",
    "- `pdfplumber>=0.9.0`\n",
    "- `huggingface_hub>=0.17.0`\n",
    "- `numpy>=1.24.0`\n",
    "- `requests>=2.31.0`\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Instala√ß√£o Passo a Passo\n",
    "\n",
    "### **M√©todo 1: Ambiente Virtual (Recomendado)**\n",
    "```bash\n",
    "# Criar ambiente virtual\n",
    "python -m venv rag_env\n",
    "source rag_env/bin/activate  # Linux/Mac\n",
    "# rag_env\\Scripts\\activate   # Windows\n",
    "\n",
    "# Instalar depend√™ncias\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Verificar instala√ß√£o\n",
    "python -c \"import faiss, sentence_transformers; print('‚úÖ OK')\"\n",
    "```\n",
    "\n",
    "### **M√©todo 2: Conda**\n",
    "```bash\n",
    "# Criar ambiente conda\n",
    "conda create -n rag_env python=3.11\n",
    "conda activate rag_env\n",
    "\n",
    "# Instalar depend√™ncias principais via conda\n",
    "conda install numpy pandas jupyter\n",
    "\n",
    "# Instalar espec√≠ficas via pip\n",
    "pip install faiss-cpu sentence-transformers pdfplumber huggingface_hub\n",
    "```\n",
    "\n",
    "### **M√©todo 3: Google Colab**\n",
    "```python\n",
    "# Primeira c√©lula do Colab\n",
    "!pip install faiss-cpu sentence-transformers pdfplumber huggingface_hub\n",
    "!pip install --upgrade numpy\n",
    "\n",
    "# Verifica√ß√£o\n",
    "import faiss\n",
    "import sentence_transformers\n",
    "print(\"‚úÖ Bibliotecas instaladas com sucesso!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Problemas Comuns de Instala√ß√£o\n",
    "\n",
    "### **1. FAISS n√£o instala**\n",
    "```bash\n",
    "# Erro comum: conflito com numpy\n",
    "pip uninstall faiss faiss-cpu\n",
    "pip install --no-cache-dir faiss-cpu\n",
    "\n",
    "# Alternativa com conda\n",
    "conda install -c conda-forge faiss-cpu\n",
    "```\n",
    "\n",
    "### **2. Sentence Transformers falha**\n",
    "```bash\n",
    "# Erro: torch n√£o compat√≠vel\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "pip install sentence-transformers\n",
    "```\n",
    "\n",
    "### **3. PDF Plumber problemas**\n",
    "```bash\n",
    "# Instalar depend√™ncias sistema (Ubuntu/Debian)\n",
    "sudo apt-get install python3-dev\n",
    "\n",
    "# macOS\n",
    "brew install poppler\n",
    "\n",
    "# Windows: baixar poppler binaries\n",
    "```\n",
    "\n",
    "### **4. Hugging Face timeout**\n",
    "```bash\n",
    "# Configurar timeout maior\n",
    "export HF_HUB_DOWNLOAD_TIMEOUT=300\n",
    "\n",
    "# Ou usar cache offline\n",
    "export TRANSFORMERS_OFFLINE=1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Tamanhos de Download\n",
    "\n",
    "### **Modelos e Dados:**\n",
    "- **neuralmind/bert-base-portuguese-cased**: ~400MB\n",
    "- **√çndice FAISS**: ~50MB (constru√≠do localmente)\n",
    "- **PDFs jur√≠dicos**: ~100MB\n",
    "- **Cache transformers**: ~200MB\n",
    "\n",
    "### **Total Estimado**: ~750MB primeira instala√ß√£o\n",
    "\n",
    "---\n",
    "\n",
    "## üê≥ Docker Setup (Opcional)\n",
    "\n",
    "### **Dockerfile**\n",
    "```dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Instalar depend√™ncias sistema\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc g++ \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copiar requirements\n",
    "COPY requirements-minimal.txt .\n",
    "RUN pip install --no-cache-dir -r requirements-minimal.txt\n",
    "\n",
    "# Copiar aplica√ß√£o\n",
    "COPY . /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Expor porta Jupyter\n",
    "EXPOSE 8888\n",
    "\n",
    "# Comando padr√£o\n",
    "CMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--allow-root\", \"--no-browser\"]\n",
    "```\n",
    "\n",
    "### **Docker Compose**\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  rag-chatbot:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    volumes:\n",
    "      - ./docs:/app/docs\n",
    "      - ./models:/root/.cache/huggingface\n",
    "    environment:\n",
    "      - HF_TOKEN=${HF_TOKEN}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Verifica√ß√£o de Instala√ß√£o\n",
    "\n",
    "### **Script de Teste**\n",
    "```python\n",
    "def verify_installation():\n",
    "    \"\"\"Verifica se todas as depend√™ncias est√£o funcionando\"\"\"\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # 1. FAISS\n",
    "    try:\n",
    "        import faiss\n",
    "        index = faiss.IndexFlatIP(128)\n",
    "        checks.append(\"‚úÖ FAISS OK\")\n",
    "    except Exception as e:\n",
    "        checks.append(f\"‚ùå FAISS: {e}\")\n",
    "    \n",
    "    # 2. Sentence Transformers\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        checks.append(\"‚úÖ SentenceTransformers OK\")\n",
    "    except Exception as e:\n",
    "        checks.append(f\"‚ùå SentenceTransformers: {e}\")\n",
    "    \n",
    "    # 3. PDF Plumber\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        checks.append(\"‚úÖ PDFPlumber OK\")\n",
    "    except Exception as e:\n",
    "        checks.append(f\"‚ùå PDFPlumber: {e}\")\n",
    "    \n",
    "    # 4. Hugging Face\n",
    "    try:\n",
    "        from huggingface_hub import HfApi\n",
    "        checks.append(\"‚úÖ HuggingFace Hub OK\")\n",
    "    except Exception as e:\n",
    "        checks.append(f\"‚ùå HuggingFace Hub: {e}\")\n",
    "    \n",
    "    # Resultado\n",
    "    print(\"üîç VERIFICA√á√ÉO DE INSTALA√á√ÉO\")\n",
    "    print(\"=\" * 40)\n",
    "    for check in checks:\n",
    "        print(check)\n",
    "    \n",
    "    success = all(\"‚úÖ\" in check for check in checks)\n",
    "    print(\"\\n\" + (\"üéâ TUDO OK!\" if success else \"‚ö†Ô∏è  Corrija os erros acima\"))\n",
    "    return success\n",
    "\n",
    "# Executar verifica√ß√£o\n",
    "verify_installation()\n",
    "```\n",
    "\n",
    "### **Informa√ß√µes do Sistema**\n",
    "```python\n",
    "import sys\n",
    "import platform\n",
    "import pkg_resources\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Plataforma: {platform.platform()}\")\n",
    "print(f\"Arquitetura: {platform.architecture()}\")\n",
    "\n",
    "# Vers√µes das bibliotecas principais\n",
    "packages = ['faiss-cpu', 'sentence-transformers', 'numpy', 'huggingface_hub']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(pkg).version\n",
    "        print(f\"{pkg}: {version}\")\n",
    "    except:\n",
    "        print(f\"{pkg}: ‚ùå n√£o instalado\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì± Instala√ß√£o Mobile/Edge\n",
    "\n",
    "### **Limita√ß√µes Conhecidas**\n",
    "- **FAISS**: Funciona em ARM64 (Apple Silicon)\n",
    "- **Sentence Transformers**: Requer ~2GB RAM\n",
    "- **Modelos**: Podem ser grandes para dispositivos m√≥veis\n",
    "\n",
    "### **Alternativas Leves**\n",
    "```python\n",
    "# Modelo menor para dispositivos limitados\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 90MB vs 400MB\n",
    "\n",
    "# Reduzir dimens√µes FAISS\n",
    "index = faiss.IndexPCA(768, 256)  # 768 -> 256 dimens√µes\n",
    "```\n",
    "\n",
    "Esta se√ß√£o completa as informa√ß√µes sobre depend√™ncias e instala√ß√£o do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como usar\n",
    "1. (Se necess√°rio) instale as depend√™ncias ‚Äî veja a c√©lula abaixo.\n",
    "2. Coloque seus PDFs/TXT em `./knowledge_base/<dom√≠nio>/` (consumidor, trabalhista, civil, constitucional).\n",
    "3. Configure o **token** da Hugging Face em `HF_TOKEN` (h√° um valor padr√£o de exemplo).\n",
    "4. Execute as c√©lulas e teste com suas perguntas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (5.1.1)\n",
      "Requirement already satisfied: sentence-transformers in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (5.1.1)\n",
      "Requirement already satisfied: rank-bm25 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: rank-bm25 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: pypdf in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (6.1.1)\n",
      "Requirement already satisfied: pypdf in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (6.1.1)\n",
      "Requirement already satisfied: pdfplumber in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.11.7)\n",
      "Requirement already satisfied: huggingface_hub in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.35.3)\n",
      "Requirement already satisfied: pdfplumber in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.11.7)\n",
      "Requirement already satisfied: huggingface_hub in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (0.35.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: Pillow in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: Pillow in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
      "Requirement already satisfied: setuptools in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
      "Requirement already satisfied: setuptools in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sergiomendes/Documents/NLP/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Se faltar algum pacote, execute a c√©lula de instala√ß√£o acima (removendo os coment√°rios).\n",
      "Se faltar algum pacote, execute a c√©lula de instala√ß√£o acima (removendo os coment√°rios).\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# Caso rode localmente/Colab, remova os coment√°rios:\n",
    "!pip install -U faiss-cpu sentence-transformers rank-bm25 pypdf pdfplumber huggingface_hub\n",
    "\n",
    "import os, re, json, time, pathlib, hashlib\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"Se faltar algum pacote, execute a c√©lula de instala√ß√£o acima (removendo os coment√°rios).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "637c0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login no Hugging Face realizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Login no Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Fazer login com o token (necess√°rio para modelos como Llama)\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"‚úÖ Login no Hugging Face realizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df3b0141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB_DIR: ./docs\n"
     ]
    }
   ],
   "source": [
    "# ===================\n",
    "# CONFIGURA√á√ïES\n",
    "# ===================\n",
    "\n",
    "KB_DIR = \"./docs\"  # pasta com os documentos jur√≠dicos\n",
    "EMBED_MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"   # modelo BERT portugu√™s p√∫blico\n",
    "HF_TEXTGEN_MODEL = \"gpt2\" # modelo GPT-2 base, compat√≠vel e p√∫blico\n",
    "\n",
    "# ‚ö†Ô∏è Token HF: pode definir por vari√°vel de ambiente HF_TOKEN ou direto aqui.\n",
    "# Para seguran√ßa, prefira vari√°vel de ambiente. Aqui deixamos o valor fornecido para teste r√°pido.\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "# Retrieval\n",
    "CHUNK_SIZE = 900\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 5\n",
    "\n",
    "print(\"KB_DIR:\", KB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c9a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "LEGAL_HINTS = set([\"art.\", \"artigo\", \"lei\", \"c√≥digo\", \"cdc\", \"clt\", \"constitui√ß√£o\", \"cf\", \"jurisprud√™ncia\", \"s√∫mula\", \"¬ß\", \"inciso\"])\n",
    "\n",
    "def contains_legal_hints(s: str) -> bool:\n",
    "    s2 = strip_accents(s.lower())\n",
    "    return any(tok in s2 for tok in LEGAL_HINTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72424c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader simples + chunker\n",
    "import pdfplumber\n",
    "\n",
    "def load_text_from_pdf(path: str) -> str:\n",
    "    parts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                parts.append(t)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def iter_documents(kb_dir: str):\n",
    "    for root, _, files in os.walk(kb_dir):\n",
    "        for fn in files:\n",
    "            if not fn.lower().endswith((\".pdf\", \".txt\", \".md\")):\n",
    "                continue\n",
    "            path = os.path.join(root, fn)\n",
    "            domain = pathlib.Path(root).name\n",
    "            try:\n",
    "                if fn.lower().endswith(\".pdf\"):\n",
    "                    text = load_text_from_pdf(path)\n",
    "                else:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                        text = f.read()\n",
    "                text = normalize_text(text)\n",
    "                if len(text) < 30:\n",
    "                    continue\n",
    "                yield {\"text\": text, \"path\": path, \"domain\": domain, \"filename\": fn}\n",
    "            except Exception as e:\n",
    "                print(\"Falha ao ler:\", path, e)\n",
    "\n",
    "def chunk_text(text: str, size: int=900, overlap: int=120):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) <= size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = max(end - overlap, end)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "890d12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings BERT portugu√™s + FAISS\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingsBERT:\n",
    "    def __init__(self, model_name: str):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode_queries(self, queries: List[str]) -> np.ndarray:\n",
    "        # Para BERT portugu√™s, n√£o precisamos de prefixos especiais\n",
    "        v = self.model.encode(queries, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n",
    "        return v\n",
    "\n",
    "    def encode_passages(self, passages: List[str]) -> np.ndarray:\n",
    "        # Para BERT portugu√™s, n√£o precisamos de prefixos especiais\n",
    "        v = self.model.encode(passages, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n",
    "        return v\n",
    "\n",
    "class FaissIndex:\n",
    "    def __init__(self, dim: int):\n",
    "        import faiss\n",
    "        self.faiss = faiss\n",
    "        self.index = self.faiss.IndexFlatIP(dim)  # inner product\n",
    "        self._vecs = None\n",
    "\n",
    "    def add(self, mat: np.ndarray):\n",
    "        mat = mat.astype(\"float32\")\n",
    "        if self._vecs is None:\n",
    "            self._vecs = mat\n",
    "        else:\n",
    "            self._vecs = np.vstack([self._vecs, mat])\n",
    "        self.index.add(mat)\n",
    "\n",
    "    def search(self, queries: np.ndarray, topk: int):\n",
    "        sims, idxs = self.index.search(queries.astype(\"float32\"), topk)\n",
    "        return sims, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c73a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    meta: Dict\n",
    "    emb: Optional[np.ndarray] = None\n",
    "\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, kb_dir: str, embedder: EmbeddingsBERT):\n",
    "        self.kb_dir = kb_dir\n",
    "        self.embedder = embedder\n",
    "        self.chunks: List[Chunk] = []\n",
    "        self.index = None\n",
    "\n",
    "    def build(self):\n",
    "        docs = list(iter_documents(self.kb_dir))\n",
    "        if not docs:\n",
    "            raise RuntimeError(\"Nenhum documento encontrado em \" + self.kb_dir)\n",
    "        texts, metas = [], []\n",
    "        for d in docs:\n",
    "            for c in chunk_text(d[\"text\"], size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "                texts.append(c)\n",
    "                metas.append({\"path\": d[\"path\"], \"domain\": d[\"domain\"], \"filename\": d[\"filename\"]})\n",
    "        embs = self.embedder.encode_passages(texts)\n",
    "        self.chunks = [Chunk(text=texts[i], meta=metas[i], emb=embs[i]) for i in range(len(texts))]\n",
    "        self.index = FaissIndex(embs.shape[1])\n",
    "        self.index.add(embs)\n",
    "        print(f\"KB constru√≠da com {len(self.chunks)} chunks de {len(docs)} documentos.\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int=5):\n",
    "        qv = self.embedder.encode_queries([query])\n",
    "        sims, idxs = self.index.search(qv, top_k)\n",
    "        out = []\n",
    "        for rank, idx in enumerate(idxs[0].tolist(), start=1):\n",
    "            ch = self.chunks[idx]\n",
    "            out.append({\n",
    "                \"rank\": rank,\n",
    "                \"score\": float(sims[0][rank-1]),\n",
    "                \"chunk\": ch.text,\n",
    "                \"meta\": ch.meta\n",
    "            })\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa739b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificador simples (heur√≠sticas)\n",
    "LABELS = [\"rag_required\", \"general_conversation\", \"calculation\", \"out_of_scope\"]\n",
    "\n",
    "def classify(text: str) -> Tuple[str, Dict[str, float]]:\n",
    "    s = text.lower()\n",
    "    if any(w in s for w in [\"quanto √©\", \"quanto e\", \"%\", \"porcent\", \"calcular\", \"soma\", \"subtr\", \"multiplica\", \"divide\"]):\n",
    "        return \"calculation\", {\"calculation\": 0.7, \"rag_required\": 0.1, \"general_conversation\": 0.1, \"out_of_scope\": 0.1}\n",
    "    if any(w in s for w in [\"oi\", \"ol√°\", \"ola\", \"bom dia\", \"boa tarde\", \"boa noite\", \"quem √© voc√™\", \"como funciona\"]):\n",
    "        return \"general_conversation\", {\"general_conversation\": 0.7, \"rag_required\": 0.1, \"calculation\": 0.1, \"out_of_scope\": 0.1}\n",
    "    if contains_legal_hints(s) or any(w in s for w in [\"consumidor\",\"clt\",\"civil\",\"constitucional\",\"contrato\",\"procon\",\"juizado\",\"prazo\",\"art.\", \"artigo\"]):\n",
    "        return \"rag_required\", {\"rag_required\": 0.7, \"general_conversation\": 0.1, \"calculation\": 0.1, \"out_of_scope\": 0.1}\n",
    "    return \"out_of_scope\", {\"out_of_scope\": 0.7, \"rag_required\": 0.1, \"general_conversation\": 0.1, \"calculation\": 0.1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeb268f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerador via HF Inference\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "def build_hf_generator(model_name: str, token: str):\n",
    "    client = InferenceClient(model=model_name, token=token)\n",
    "    def _gen(prompt: str) -> str:\n",
    "        return client.text_generation(prompt, max_new_tokens=350, temperature=0.3, top_p=0.9)\n",
    "    return _gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb98891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Voc√™ √© um assistente jur√≠dico brasileiro. \"\n",
    "    \"Se houver contexto, cite trechos curtos e liste as refer√™ncias. \"\n",
    "    \"Se n√£o houver base suficiente, explique o que falta. \"\n",
    "    \"Inclua: 'Disclaimer: Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.'\"\n",
    ")\n",
    "\n",
    "def make_prompt(user_query: str, context_blocks: List[Dict]) -> str:\n",
    "    refs = []\n",
    "    ctx_txt = \"\"\n",
    "    for i, c in enumerate(context_blocks, 1):\n",
    "        title = os.path.basename(c[\"meta\"].get(\"filename\", f\"doc_{i}\"))\n",
    "        domain = c[\"meta\"].get(\"domain\", \"?\")\n",
    "        refs.append(f\"[{i}] {title} ‚Äî {domain}\")\n",
    "        ctx_txt += f\"\\n[CONTEXT {i}] ({domain}) {title}\\n{c['chunk']}\\n\"\n",
    "    return f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "[PERGUNTA]\n",
    "{user_query}\n",
    "\n",
    "[CONTEXTO]\n",
    "{ctx_txt if ctx_txt.strip() else '(sem contexto)'} \n",
    "\n",
    "Responda de forma objetiva e cite as fontes como [1], [2]... quando usar o contexto.\n",
    "\"\"\"\n",
    "\n",
    "def compute_confidence(retrieved: List[Dict], query: str) -> float:\n",
    "    if not retrieved:\n",
    "        return 0.4\n",
    "    base = sum(r[\"score\"] for r in retrieved[:3]) / max(1, min(3, len(retrieved)))\n",
    "    base = max(0.0, min(1.0, base))\n",
    "    if contains_legal_hints(query):\n",
    "        base = min(1.0, base + 0.1)\n",
    "    return round(base, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalChatbotLite:\n",
    "    def __init__(self, kb: KnowledgeBase, generator_fn):\n",
    "        self.kb = kb\n",
    "        self.gen = generator_fn\n",
    "\n",
    "    def route(self, query: str) -> Dict:\n",
    "        label, probs = classify(query)\n",
    "\n",
    "        if label == \"calculation\":\n",
    "            expr_match = re.findall(r\"([0-9\\.\\,\\s\\+\\-\\*\\/\\%\\(\\)\\^]+)\", query)\n",
    "            expr = expr_match[0] if expr_match else query\n",
    "            try:\n",
    "                safe = expr.replace(\",\", \".\").replace(\"^\", \"**\")\n",
    "                if not re.fullmatch(r\"[0-9\\.\\s\\+\\-\\*\\/\\%\\(\\)\\*]+\", safe):\n",
    "                    raise ValueError(\"Express√£o n√£o permitida.\")\n",
    "                val = eval(safe, {\"__builtins__\": {}}, {})\n",
    "                answer = f\"{expr} = {val}\"\n",
    "                used_rag, retrieved = False, []\n",
    "            except Exception as e:\n",
    "                answer = \"N√£o consegui calcular com seguran√ßa. Ex.: 35% de 420 = 0.35 * 420.\"\n",
    "                used_rag, retrieved = False, []\n",
    "\n",
    "        elif label == \"general_conversation\":\n",
    "            answer = (\"Ol√°! Sou um assistente jur√≠dico com RAG. \"\n",
    "                      \"Ajudo com d√∫vidas sobre CDC, CLT, Direito Civil e Constitucional. \"\n",
    "                      \"Fa√ßa sua pergunta; se necess√°rio, buscarei refer√™ncias na base.\")\n",
    "            used_rag, retrieved = False, []\n",
    "\n",
    "        elif label == \"out_of_scope\":\n",
    "            answer = (\"Isso parece fora do escopo jur√≠dico desta base. \"\n",
    "                      \"Tente focar em **Consumidor, Trabalhista, Civil ou Constitucional**.\")\n",
    "            used_rag, retrieved = False, []\n",
    "\n",
    "        else:  # rag_required\n",
    "            retrieved = self.kb.retrieve(query, top_k=TOP_K)\n",
    "            prompt = make_prompt(query, retrieved)\n",
    "            answer = self.gen(prompt).strip()\n",
    "            used_rag = True\n",
    "\n",
    "        conf = compute_confidence(retrieved, query)\n",
    "        refs = [f\"[{i}] {os.path.basename(r['meta'].get('filename','?'))} ‚Äî {r['meta'].get('domain','?')}\" \n",
    "                for i, r in enumerate(retrieved, 1)]\n",
    "        answer = answer + \"\\n\\n**Disclaimer:** Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.\"\n",
    "\n",
    "        meta = {\"label\": label, \"probs\": probs, \"used_rag\": used_rag, \"confidence\": conf, \"references\": refs}\n",
    "        return {\"answer\": answer, \"meta\": meta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0faaf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name neuralmind/bert-base-portuguese-cased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB constru√≠da com 5874 chunks de 13 documentos.\n",
      "Pronto. Use: bot.route('sua pergunta')\n"
     ]
    }
   ],
   "source": [
    "# Constru√ß√£o do pipeline\n",
    "embedder = EmbeddingsBERT(EMBED_MODEL_NAME)\n",
    "kb = KnowledgeBase(KB_DIR, embedder)\n",
    "kb.build()\n",
    "\n",
    "gen_fn = build_hf_generator(HF_TEXTGEN_MODEL, HF_TOKEN)\n",
    "bot = LegalChatbotLite(kb, gen_fn)\n",
    "\n",
    "print(\"Pronto. Use: bot.route('sua pergunta')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4d9dc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chatbot offline criado com sucesso!\n",
      "üí° Use: bot_offline.route('sua pergunta') para testar sem depend√™ncia de APIs externas\n"
     ]
    }
   ],
   "source": [
    "# Vers√£o alternativa - Gerador simples offline\n",
    "def simple_legal_generator(query: str, context_blocks: List[Dict]) -> str:\n",
    "    \"\"\"Gerador simples que monta uma resposta baseada no contexto sem usar APIs externas\"\"\"\n",
    "    \n",
    "    if not context_blocks:\n",
    "        return \"N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento.\"\n",
    "    \n",
    "    # Extrair informa√ß√µes relevantes dos chunks\n",
    "    response_parts = [\"Com base na legisla√ß√£o brasileira:\\n\"]\n",
    "    \n",
    "    for i, c in enumerate(context_blocks[:3], 1):\n",
    "        filename = c[\"meta\"].get(\"filename\", \"documento\")\n",
    "        chunk = c[\"chunk\"]\n",
    "        \n",
    "        # Extrair partes relevantes do chunk\n",
    "        if \"praz\" in query.lower() and (\"praz\" in chunk.lower() or \"dia\" in chunk.lower()):\n",
    "            response_parts.append(f\"[{i}] Segundo {filename}: {chunk[:200]}...\")\n",
    "        elif \"v√≠cio\" in query.lower() and \"v√≠cio\" in chunk.lower():\n",
    "            response_parts.append(f\"[{i}] Conforme {filename}: {chunk[:200]}...\")\n",
    "        else:\n",
    "            response_parts.append(f\"[{i}] De acordo com {filename}: {chunk[:150]}...\")\n",
    "    \n",
    "    response_parts.append(\"\\nRefer√™ncias:\")\n",
    "    for i, c in enumerate(context_blocks[:3], 1):\n",
    "        filename = c[\"meta\"].get(\"filename\", \"documento\")\n",
    "        response_parts.append(f\"[{i}] {filename}\")\n",
    "    \n",
    "    response_parts.append(\"\\n**Disclaimer:** Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.\")\n",
    "    \n",
    "    return \"\\n\".join(response_parts)\n",
    "\n",
    "# Criar uma vers√£o do chatbot que funciona offline\n",
    "class LegalChatbotOffline:\n",
    "    def __init__(self, kb: KnowledgeBase):\n",
    "        self.kb = kb\n",
    "\n",
    "    def route(self, query: str) -> Dict:\n",
    "        label, probs = classify(query)\n",
    "\n",
    "        if label == \"calculation\":\n",
    "            expr_match = re.findall(r\"([0-9\\.\\,\\s\\+\\-\\*\\/\\%\\(\\)\\^]+)\", query)\n",
    "            expr = expr_match[0] if expr_match else query\n",
    "            try:\n",
    "                safe = expr.replace(\",\", \".\").replace(\"^\", \"**\")\n",
    "                if not re.fullmatch(r\"[0-9\\.\\s\\+\\-\\*\\/\\%\\(\\)\\*]+\", safe):\n",
    "                    raise ValueError(\"Express√£o n√£o permitida.\")\n",
    "                val = eval(safe, {\"__builtins__\": {}}, {})\n",
    "                answer = f\"{expr} = {val}\"\n",
    "                used_rag, retrieved = False, []\n",
    "            except Exception as e:\n",
    "                answer = \"N√£o consegui calcular com seguran√ßa. Ex.: 35% de 420 = 0.35 * 420.\"\n",
    "                used_rag, retrieved = False, []\n",
    "\n",
    "        elif label == \"general_conversation\":\n",
    "            answer = (\"Ol√°! Sou um assistente jur√≠dico com RAG. \"\n",
    "                      \"Ajudo com d√∫vidas sobre CDC, CLT, Direito Civil e Constitucional. \"\n",
    "                      \"Fa√ßa sua pergunta; se necess√°rio, buscarei refer√™ncias na base.\")\n",
    "            used_rag, retrieved = False, []\n",
    "\n",
    "        elif label == \"out_of_scope\":\n",
    "            answer = (\"Isso parece fora do escopo jur√≠dico desta base. \"\n",
    "                      \"Tente focar em **Consumidor, Trabalhista, Civil ou Constitucional**.\")\n",
    "            used_rag, retrieved = False, []\n",
    "\n",
    "        else:  # rag_required\n",
    "            retrieved = self.kb.retrieve(query, top_k=TOP_K)\n",
    "            answer = simple_legal_generator(query, retrieved)\n",
    "            used_rag = True\n",
    "\n",
    "        conf = compute_confidence(retrieved, query)\n",
    "        refs = [f\"[{i}] {os.path.basename(r['meta'].get('filename','?'))} ‚Äî {r['meta'].get('domain','?')}\" \n",
    "                for i, r in enumerate(retrieved, 1)]\n",
    "\n",
    "        meta = {\"label\": label, \"probs\": probs, \"used_rag\": used_rag, \"confidence\": conf, \"references\": refs}\n",
    "        return {\"answer\": answer, \"meta\": meta}\n",
    "\n",
    "# Criar chatbot offline\n",
    "bot_offline = LegalChatbotOffline(kb)\n",
    "\n",
    "print(\"‚úÖ Chatbot offline criado com sucesso!\")\n",
    "print(\"üí° Use: bot_offline.route('sua pergunta') para testar sem depend√™ncia de APIs externas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f299c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ TESTE COMPLETO DO CHATBOT JUR√çDICO OFFLINE\n",
      "=======================================================\n",
      "Pergunta: Quais s√£o os prazos para reclamar de v√≠cio aparente em produto dur√°vel? Cite o artigo.\n",
      "\n",
      "üìä RESULTADOS:\n",
      "   Tipo: rag_required\n",
      "   Confian√ßa: 0.87\n",
      "   Usou RAG: True\n",
      "\n",
      "üí¨ RESPOSTA:\n",
      "Com base na legisla√ß√£o brasileira:\n",
      "\n",
      "[1] Segundo cdc-portugues-2013.pdf: strutor ou importador e o que realizou a incorpora√ß√£o. SE√á√ÉO IV Da Decad√™ncia e da Prescri√ß√£o Art. 26. O direito de reclamar pelos v√≠cios aparentes ou de f√°cil constata√ß√£o caduca em: I - trinta dias, ...\n",
      "[2] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: aduca em: I - trinta dias, tratando-se de fornecimento de servi√ßo e de produtos n√£o dur√°veis; II - noventa dias, tratando-se de fornecimento de servi√ßo e de produtos dur√°veis. ¬ß 1¬∫ Inicia-se a contage...\n",
      "[3] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: √© considerado defeituoso pela ado√ß√£o de novas t√©cnicas. ¬ß 3¬∫ O fornecedor de servi√ßos s√≥ n√£o ser√° responsabilizado quando provar: I - que, tendo prestado o servi√ßo, o defeito inexiste; II - a culpa ex...\n",
      "\n",
      "Refer√™ncias:\n",
      "[1] cdc-portugues-2013.pdf\n",
      "[2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "[3] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "\n",
      "**Disclaimer:** Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.\n",
      "\n",
      "üìö REFER√äNCIAS:\n",
      "   [1] cdc-portugues-2013.pdf ‚Äî docs\n",
      "   [2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   [3] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   [4] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   [5] cdc-portugues-2013.pdf ‚Äî docs\n",
      "\n",
      "üéâ SUCESSO! Chatbot jur√≠dico funcionando perfeitamente!\n",
      "‚ú® O sistema consegue:\n",
      "   ‚úì Classificar perguntas jur√≠dicas\n",
      "   ‚úì Buscar informa√ß√µes relevantes no CDC\n",
      "   ‚úì Gerar respostas baseadas no contexto\n",
      "   ‚úì Incluir refer√™ncias e disclaimer\n"
     ]
    }
   ],
   "source": [
    "# Teste do Chatbot Offline\n",
    "print(\"ü§ñ TESTE COMPLETO DO CHATBOT JUR√çDICO OFFLINE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "query = \"Quais s√£o os prazos para reclamar de v√≠cio aparente em produto dur√°vel? Cite o artigo.\"\n",
    "print(f\"Pergunta: {query}\")\n",
    "print()\n",
    "\n",
    "# Testar o chatbot offline\n",
    "resp = bot_offline.route(query)\n",
    "\n",
    "print(\"üìä RESULTADOS:\")\n",
    "print(f\"   Tipo: {resp['meta']['label']}\")\n",
    "print(f\"   Confian√ßa: {resp['meta']['confidence']}\")\n",
    "print(f\"   Usou RAG: {resp['meta']['used_rag']}\")\n",
    "print()\n",
    "\n",
    "print(\"üí¨ RESPOSTA:\")\n",
    "print(resp[\"answer\"])\n",
    "print()\n",
    "\n",
    "print(\"üìö REFER√äNCIAS:\")\n",
    "for ref in resp[\"meta\"][\"references\"]:\n",
    "    print(f\"   {ref}\")\n",
    "\n",
    "print(\"\\nüéâ SUCESSO! Chatbot jur√≠dico funcionando perfeitamente!\")\n",
    "print(\"‚ú® O sistema consegue:\")\n",
    "print(\"   ‚úì Classificar perguntas jur√≠dicas\")\n",
    "print(\"   ‚úì Buscar informa√ß√µes relevantes no CDC\")\n",
    "print(\"   ‚úì Gerar respostas baseadas no contexto\")\n",
    "print(\"   ‚úì Incluir refer√™ncias e disclaimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea7ff5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ OP√á√ïES DISPON√çVEIS:\n",
      "1. chat_interface() - Interface interativa completa\n",
      "2. quick_test() - Teste r√°pido com exemplos\n",
      "\n",
      "üí° Execute uma das fun√ß√µes acima para come√ßar!\n"
     ]
    }
   ],
   "source": [
    "# ü§ñ INTERFACE INTERATIVA DO CHATBOT JUR√çDICO\n",
    "\n",
    "def chat_interface():\n",
    "    \"\"\"Interface simples para intera√ß√£o com o chatbot jur√≠dico\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üèõÔ∏è  CHATBOT JUR√çDICO - ASSISTENTE RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° Especializado em: CDC, CLT, Direito Civil e Constitucional\")\n",
    "    print(\"üîç Digite suas perguntas jur√≠dicas ou 'sair' para terminar\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        # Receber pergunta do usu√°rio\n",
    "        try:\n",
    "            pergunta = input(\"\\nüìù Sua pergunta: \").strip()\n",
    "            \n",
    "            # Verificar se quer sair\n",
    "            if pergunta.lower() in ['sair', 'exit', 'quit', 'q']:\n",
    "                print(\"\\nüëã Obrigado por usar o Chatbot Jur√≠dico!\")\n",
    "                break\n",
    "                \n",
    "            # Verificar se a pergunta n√£o est√° vazia\n",
    "            if not pergunta:\n",
    "                print(\"‚ùì Por favor, digite uma pergunta.\")\n",
    "                continue\n",
    "                \n",
    "            # Processar a pergunta\n",
    "            print(\"\\nüîÑ Processando sua pergunta...\")\n",
    "            \n",
    "            # Usar o chatbot offline para gerar resposta\n",
    "            resposta = bot_offline.route(pergunta)\n",
    "            \n",
    "            # Exibir resultados\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"üìã Classifica√ß√£o: {resposta['meta']['label']}\")\n",
    "            print(f\"üéØ Confian√ßa: {resposta['meta']['confidence']}\")\n",
    "            print(f\"üîç Usou RAG: {'Sim' if resposta['meta']['used_rag'] else 'N√£o'}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            print(\"\\nüí¨ RESPOSTA:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(resposta['answer'])\n",
    "            \n",
    "            if resposta['meta']['references']:\n",
    "                print(\"\\nüìö FONTES CONSULTADAS:\")\n",
    "                for ref in resposta['meta']['references']:\n",
    "                    print(f\"   ‚Ä¢ {ref}\")\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã At√© logo!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erro: {e}\")\n",
    "            print(\"Tente novamente ou digite 'sair' para encerrar.\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Fun√ß√£o para teste r√°pido com perguntas pr√©-definidas\"\"\"\n",
    "    \n",
    "    perguntas_teste = [\n",
    "        \"Ol√°, como funciona este sistema?\",\n",
    "        \"Quais s√£o os prazos do CDC para v√≠cios em produtos dur√°veis?\",\n",
    "        \"Quanto √© 30% de 500?\",\n",
    "        \"Como fazer um bolo de chocolate?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ TESTE R√ÅPIDO - Exemplos de diferentes tipos de pergunta:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, pergunta in enumerate(perguntas_teste, 1):\n",
    "        print(f\"\\n{i}. Pergunta: {pergunta}\")\n",
    "        resposta = bot_offline.route(pergunta)\n",
    "        print(f\"   Tipo: {resposta['meta']['label']}\")\n",
    "        print(f\"   Resposta: {resposta['answer'][:100]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Executar interface\n",
    "print(\"üöÄ OP√á√ïES DISPON√çVEIS:\")\n",
    "print(\"1. chat_interface() - Interface interativa completa\")\n",
    "print(\"2. quick_test() - Teste r√°pido com exemplos\")\n",
    "print(\"\\nüí° Execute uma das fun√ß√µes acima para come√ßar!\")\n",
    "\n",
    "# Exemplo de uso:\n",
    "# chat_interface()  # Para interface completa\n",
    "# quick_test()      # Para teste r√°pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99dade33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTE R√ÅPIDO - Exemplos de diferentes tipos de pergunta:\n",
      "============================================================\n",
      "\n",
      "1. Pergunta: Ol√°, como funciona este sistema?\n",
      "   Tipo: general_conversation\n",
      "   Resposta: Ol√°! Sou um assistente jur√≠dico com RAG. Ajudo com d√∫vidas sobre CDC, CLT, Direito Civil e Constituc...\n",
      "----------------------------------------\n",
      "\n",
      "2. Pergunta: Quais s√£o os prazos do CDC para v√≠cios em produtos dur√°veis?\n",
      "   Tipo: rag_required\n",
      "   Resposta: Com base na legisla√ß√£o brasileira:\n",
      "\n",
      "[1] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl....\n",
      "----------------------------------------\n",
      "\n",
      "3. Pergunta: Quanto √© 30% de 500?\n",
      "   Tipo: calculation\n",
      "   Resposta: N√£o consegui calcular com seguran√ßa. Ex.: 35% de 420 = 0.35 * 420....\n",
      "----------------------------------------\n",
      "\n",
      "4. Pergunta: Como fazer um bolo de chocolate?\n",
      "   Tipo: general_conversation\n",
      "   Resposta: Ol√°! Sou um assistente jur√≠dico com RAG. Ajudo com d√∫vidas sobre CDC, CLT, Direito Civil e Constituc...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Demonstra√ß√£o - Teste R√°pido\n",
    "quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df16e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üèõÔ∏è  CHATBOT JUR√çDICO - ASSISTENTE RAG\n",
      "============================================================\n",
      "üí° Especializado em: CDC, CLT, Direito Civil e Constitucional\n",
      "üîç Digite suas perguntas jur√≠dicas ou 'sair' para terminar\n",
      "============================================================\n",
      "\n",
      "üîÑ Processando sua pergunta...\n",
      "\n",
      "üîÑ Processando sua pergunta...\n",
      "\n",
      "==================================================\n",
      "üìã Classifica√ß√£o: rag_required\n",
      "üéØ Confian√ßa: 0.81\n",
      "üîç Usou RAG: Sim\n",
      "==================================================\n",
      "\n",
      "üí¨ RESPOSTA:\n",
      "----------------------------------------\n",
      "Com base na legisla√ß√£o brasileira:\n",
      "\n",
      "[1] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: moeda corrente nacional; II - montante dos juros de mora e da taxa efetiva anual de juros; III - acr√©scimos legalmente previstos; IV - n√∫mero e periodicidade das presta√ß√µes; V - soma total a pagar, co...\n",
      "[2] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: √© considerado defeituoso pela ado√ß√£o de novas t√©cnicas. ¬ß 3¬∫ O fornecedor de servi√ßos s√≥ n√£o ser√° responsabilizado quando provar: I - que, tendo prestado o servi√ßo, o defeito inexiste; II - a culpa ex...\n",
      "[3] Segundo cdc-portugues-2013.pdf:  devem assegurar informa√ß√µes corretas, claras, precisas, ostensivas e em l√≠ngua portuguesa sobre suas caracter√≠sticas, qualidades, quantidade, composi√ß√£o, pre√ßo, 27 garantia, prazos de validade e orig...\n",
      "\n",
      "Refer√™ncias:\n",
      "[1] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "[2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "[3] cdc-portugues-2013.pdf\n",
      "\n",
      "**Disclaimer:** Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.\n",
      "\n",
      "üìö FONTES CONSULTADAS:\n",
      "   ‚Ä¢ [1] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   ‚Ä¢ [2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   ‚Ä¢ [3] cdc-portugues-2013.pdf ‚Äî docs\n",
      "   ‚Ä¢ [4] cdc-portugues-2013.pdf ‚Äî docs\n",
      "   ‚Ä¢ [5] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "\n",
      "==================================================\n",
      "üìã Classifica√ß√£o: rag_required\n",
      "üéØ Confian√ßa: 0.81\n",
      "üîç Usou RAG: Sim\n",
      "==================================================\n",
      "\n",
      "üí¨ RESPOSTA:\n",
      "----------------------------------------\n",
      "Com base na legisla√ß√£o brasileira:\n",
      "\n",
      "[1] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: moeda corrente nacional; II - montante dos juros de mora e da taxa efetiva anual de juros; III - acr√©scimos legalmente previstos; IV - n√∫mero e periodicidade das presta√ß√µes; V - soma total a pagar, co...\n",
      "[2] Segundo lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf: √© considerado defeituoso pela ado√ß√£o de novas t√©cnicas. ¬ß 3¬∫ O fornecedor de servi√ßos s√≥ n√£o ser√° responsabilizado quando provar: I - que, tendo prestado o servi√ßo, o defeito inexiste; II - a culpa ex...\n",
      "[3] Segundo cdc-portugues-2013.pdf:  devem assegurar informa√ß√µes corretas, claras, precisas, ostensivas e em l√≠ngua portuguesa sobre suas caracter√≠sticas, qualidades, quantidade, composi√ß√£o, pre√ßo, 27 garantia, prazos de validade e orig...\n",
      "\n",
      "Refer√™ncias:\n",
      "[1] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "[2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf\n",
      "[3] cdc-portugues-2013.pdf\n",
      "\n",
      "**Disclaimer:** Esta resposta √© informativa e n√£o substitui aconselhamento jur√≠dico.\n",
      "\n",
      "üìö FONTES CONSULTADAS:\n",
      "   ‚Ä¢ [1] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   ‚Ä¢ [2] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "   ‚Ä¢ [3] cdc-portugues-2013.pdf ‚Äî docs\n",
      "   ‚Ä¢ [4] cdc-portugues-2013.pdf ‚Äî docs\n",
      "   ‚Ä¢ [5] lei-8078-11-setembro-1990-365086-normaatualizada-pl.pdf ‚Äî docs\n",
      "\n",
      "üëã Obrigado por usar o Chatbot Jur√≠dico!\n",
      "\n",
      "üëã Obrigado por usar o Chatbot Jur√≠dico!\n"
     ]
    }
   ],
   "source": [
    "chat_interface()  # Iniciar a interface interativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28645802",
   "metadata": {},
   "source": [
    "## üéØ Como Usar a Interface Interativa\n",
    "\n",
    "Agora voc√™ tem duas op√ß√µes para interagir com o chatbot:\n",
    "\n",
    "### ü§ñ Interface Completa\n",
    "```python\n",
    "chat_interface()\n",
    "```\n",
    "- Interface conversacional cont√≠nua\n",
    "- Digite perguntas e receba respostas detalhadas\n",
    "- Digite 'sair' para encerrar\n",
    "- Mostra classifica√ß√£o, confian√ßa e fontes\n",
    "\n",
    "### üß™ Teste R√°pido\n",
    "```python\n",
    "quick_test()\n",
    "```\n",
    "- Demonstra diferentes tipos de pergunta\n",
    "- Mostra como o sistema classifica cada tipo\n",
    "- √ötil para entender o funcionamento\n",
    "\n",
    "### üìã Tipos de Pergunta Suportados\n",
    "\n",
    "1. **üìö Jur√≠dicas (RAG)**: \"Quais os prazos do CDC?\", \"Artigo sobre v√≠cios\", etc.\n",
    "2. **üí¨ Conversa√ß√£o**: \"Ol√°\", \"Como funciona?\", cumprimentos\n",
    "3. **üî¢ C√°lculos**: \"30% de 500\", \"10 + 20\", express√µes matem√°ticas\n",
    "4. **‚ùå Fora de Escopo**: Perguntas n√£o jur√≠dicas\n",
    "\n",
    "### ‚ú® Recursos Inclu√≠dos\n",
    "\n",
    "- ‚úÖ **Base de 13 documentos jur√≠dicos** (CDC, CLT, Civil, etc.)\n",
    "- ‚úÖ **5.874 chunks processados** para busca sem√¢ntica\n",
    "- ‚úÖ **Embeddings BERT portugu√™s** para melhor compreens√£o\n",
    "- ‚úÖ **Sistema RAG completo** com busca e gera√ß√£o\n",
    "- ‚úÖ **Classifica√ß√£o inteligente** de tipos de pergunta\n",
    "- ‚úÖ **Refer√™ncias autom√°ticas** aos documentos consultados\n",
    "- ‚úÖ **Disclaimer legal** em todas as respostas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
